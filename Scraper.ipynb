{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Diddy Sheets\n",
    "First column = date\n",
    "Second column = home team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas scikit-learn bs4 requests\n",
    "\n",
    "import pandas as pd \n",
    "from datetime import datetime, date\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opp_team(soup: BeautifulSoup) -> str: \n",
    "    scorebox_div = soup.find('div', class_=\"scorebox\")\n",
    "    if scorebox_div is None:\n",
    "        print(\"Not Found\")\n",
    "        return None\n",
    "    \n",
    "    link_tag = scorebox_div.find('strong').find('a')\n",
    "    if link_tag is None:\n",
    "        print(\"<a> tag not found\")\n",
    "        return None\n",
    "\n",
    "    if 'href' not in link_tag.attrs:\n",
    "        print(\"Link not found in href\")\n",
    "        return None\n",
    "    \n",
    "    href = link_tag[\"href\"]\n",
    "\n",
    "    return href.split(\"/\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_headers(soup: BeautifulSoup, table_id: str) -> str:\n",
    "    table = soup.find('table', table_id)\n",
    "\n",
    "    if table is None:\n",
    "        print(\"Couldn't find the table\")\n",
    "        return None\n",
    "    \n",
    "    table_header = table.find('thead')\n",
    "\n",
    "    if table_header is None:\n",
    "        print(\"Couldn't find the table header\")\n",
    "    \n",
    "    data_stats = [th['data-stat'] for th in table.find_all('th') if 'data_stat' in th.attrs]\n",
    "\n",
    "    return data_stats[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stat_table(soup: BeautifulSoup, table_id: str, team: str) -> pd.DataFrame:\n",
    "    table = soup.find('table', id = table_id)\n",
    "\n",
    "    if table is None:\n",
    "        print(\"Table not found\")\n",
    "        return None\n",
    "    \n",
    "    table_body = table.find('tbody')\n",
    "\n",
    "    if table_body is None:\n",
    "        print(\"Table body not found\")\n",
    "        return None\n",
    "\n",
    "    table_rows = [tr for tr in table_body.find_all('tr') if 'thead' not in tr.attrs]\n",
    "\n",
    "    if table_rows is None:\n",
    "        print(\"Table rows not found\")\n",
    "        return None\n",
    "\n",
    "    data = []\n",
    "    col_names = None\n",
    "\n",
    "    for row in table_rows:\n",
    "        table_data = [td for td in row if td.name == \"td\"]\n",
    "\n",
    "        if \"Did Not Play\" in str(row) or \"Reserves\" in str(row): \n",
    "            continue \n",
    "    \n",
    "        player_name = row.find('th').text.strip()\n",
    "\n",
    "        if col_names is None:\n",
    "            col_names = [\"player\", \"team\"] + [td['data-stat'] for td in table_data if 'data-stat' in td.attrs]\n",
    "        \n",
    "        data_values = [td.text.strip() for td in table_data]\n",
    "        data_values = [player_name, team] + data_values\n",
    "        data.append(data_values)\n",
    "\n",
    "    if col_names and data:\n",
    "        return pd.DataFrame(data, columns=col_names)\n",
    "    else:\n",
    "        print(\"No data found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_date(df: pd.DataFrame, date: date):\n",
    "    df['date'] = date\n",
    "    return df\n",
    "\n",
    "def correct_date(df: pd.DataFrame):\n",
    "    if \"date\" not in df.columns:\n",
    "        print(\"Dates not found. Dataframe must contain a column named 'date'\")\n",
    "        return None\n",
    "    \n",
    "    if pd.api.types.is_string_dtype(df['date']):\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the home team and the day the game was played\n",
    "def scrape_game(home_team: str, game_date: date, sec_delay: int):\n",
    "    print(f\"Scraping: {home_team}\")\n",
    "\n",
    "    time.sleep(sec_delay)\n",
    "\n",
    "    date_str = game_date.strftime(\"%Y%m%d\")\n",
    "    link = f\"https://www.basketball-reference.com/boxscores/{date_str}0{home_team}.html\"\n",
    "\n",
    "    response = requests.get(link)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Not Found: \" + link)\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    opp_team = get_opp_team(soup)\n",
    "\n",
    "    home_table_id = f\"box-{home_team}-game-basic\"\n",
    "    opponent_table_id = f\"box-{opp_team}-game-basic\"\n",
    "\n",
    "    home_df = get_stat_table(soup, home_table_id, home_team)\n",
    "    home_df = add_date(home_df, game_date)\n",
    "    opp_df = get_stat_table(soup, opponent_table_id, opp_team)\n",
    "    opp_df = add_date(opp_df, game_date)\n",
    "\n",
    "    return pd.concat([home_df, opp_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_sheets(df: pd.DataFrame, sec_delay: int) -> pd.DataFrame:\n",
    "    all_games_df = df.apply(lambda row: scrape_game(row['home_team'], row['date'], sec_delay=sec_delay), axis = 1).tolist()\n",
    "    \n",
    "    return pd.concat(all_games_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the diddy csv\n",
    "diddy_path = \"./Celeb-Attendence/diddy.csv\"\n",
    "\n",
    "diddy_csv = pd.read_csv(diddy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diddy_csv.head()\n",
    "type(diddy_csv['date'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "diddy_csv = correct_date(diddy_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: LAL\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m diddy_games_df \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_sheets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiddy_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msec_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m diddy_games_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m, in \u001b[0;36mscrape_sheets\u001b[0;34m(df, sec_delay)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_sheets\u001b[39m(df: pd\u001b[38;5;241m.\u001b[39mDataFrame, sec_delay: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m----> 2\u001b[0m     all_games_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscrape_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhome_team\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msec_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msec_delay\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(all_games_df)\n",
      "File \u001b[0;32m~/BSAFall2024/DiddyProject/.venv/lib/python3.12/site-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/BSAFall2024/DiddyProject/.venv/lib/python3.12/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/BSAFall2024/DiddyProject/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m~/BSAFall2024/DiddyProject/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m, in \u001b[0;36mscrape_sheets.<locals>.<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_sheets\u001b[39m(df: pd\u001b[38;5;241m.\u001b[39mDataFrame, sec_delay: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m----> 2\u001b[0m     all_games_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[43mscrape_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhome_team\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msec_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msec_delay\u001b[49m\u001b[43m)\u001b[49m, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(all_games_df)\n",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m, in \u001b[0;36mscrape_game\u001b[0;34m(home_team, game_date, sec_delay)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_game\u001b[39m(home_team: \u001b[38;5;28mstr\u001b[39m, game_date: date, sec_delay: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhome_team\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msec_delay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     date_str \u001b[38;5;241m=\u001b[39m game_date\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     link \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.basketball-reference.com/boxscores/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhome_team\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "diddy_games_df = scrape_sheets(diddy_csv, sec_delay=5)\n",
    "diddy_games_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "diddy_save_path = \"./Diddy-Stats/diddygames.csv\"\n",
    "\n",
    "diddy_games_df.to_csv(diddy_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Career Stats of the Top 20 Diddy players\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_player_links = {\n",
    "    \"Dwyane Wade\": \"https://www.basketball-reference.com/players/w/wadedw01.html\",\n",
    "    \"Kobe Bryant\": \"https://www.basketball-reference.com/players/b/bryanko01.html\",\n",
    "    \"Lamar Odom\": \"https://www.basketball-reference.com/players/o/odomla01.html\",\n",
    "    \"Pau Gasol\": \"https://www.basketball-reference.com/players/g/gasolpa01.html\",\n",
    "    \"Derek Fisher\": \"https://www.basketball-reference.com/players/f/fishede01.html\",\n",
    "    \"Lebron James\": \"https://www.basketball-reference.com/players/j/jamesle01.html\",\n",
    "    \"Jordan Farmar\": \"https://www.basketball-reference.com/players/f/farmajo01.html\",\n",
    "    \"Sasha Vujačić\": \"https://www.basketball-reference.com/players/v/vujacsa01.html\",\n",
    "    \"Trevor Ariza\": \"https://www.basketball-reference.com/players/a/arizatr01.html\",\n",
    "    \"Udonis Haslem\": \"https://www.basketball-reference.com/players/h/hasleud01.html\",\n",
    "    \"Andrew Bynum\": \"https://www.basketball-reference.com/players/b/bynuman01.html\",\n",
    "    \"Jason Kidd\": \"https://www.basketball-reference.com/players/k/kiddja01.html\",\n",
    "    \"Kevin Garnett\": \"https://www.basketball-reference.com/players/g/garneke01.html\",\n",
    "    \"Luke Walton\": \"https://www.basketball-reference.com/players/w/waltolu01.html\",\n",
    "    \"Paul Pierce\": \"https://www.basketball-reference.com/players/p/piercpa01.html\",\n",
    "    \"Rajon Rondo\": \"https://www.basketball-reference.com/players/r/rondora01.html\",\n",
    "    \"Ray Allen\": \"https://www.basketball-reference.com/players/a/allenra02.html\",\n",
    "    \"Shannon Brown\": \"https://www.basketball-reference.com/players/b/brownsh01.html\",\n",
    "    \"J.R. Smith\": \"https://www.basketball-reference.com/players/s/smithjr01.html\",\n",
    "    \"Chris Bosh\": \"https://www.basketball-reference.com/players/b/boshch01.html\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New setup for the tables in basketball reference\n",
    "* regular season is in the div with id=\"div_per_game_stats\"\n",
    "* post season is in the div with id=\"div_per_game_stats_post\"\n",
    "\n",
    "For total stats\n",
    "* regular = id=\"div_totals\"\n",
    "* playoffs = \"div_playoffs_totals\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_totals(link: str, player_name: str, is_playoff: bool = False) -> pd.DataFrame:\n",
    "    response = requests.get(link)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"Not Found: \" + link)\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    if is_playoff:\n",
    "        div_id = \"div_playoffs_totals\"\n",
    "    else:\n",
    "        div_id = \"div_totals\"\n",
    "\n",
    "    div = soup.find('div', id=div_id)\n",
    "    table = div.find('table')\n",
    "\n",
    "    table_body = table.find('tbody')\n",
    "    table_rows = table_body.find_all('tr')\n",
    "    \n",
    "    data = []\n",
    "    col_names = None\n",
    "\n",
    "    for row in table_rows:\n",
    "        table_data = [td for td in row if td.name == \"td\"]\n",
    "\n",
    "        season = row.find('th').text.strip()\n",
    "\n",
    "        if col_names is None:\n",
    "            col_names = [\"player\", \"season\", \"season_type\"] + [td['data-stat'] for td in table_data if 'data-stat' in td.attrs]\n",
    "        \n",
    "        data_values = [td.text.strip() for td in table_data]\n",
    "        data_values = [player_name, season, (\"playoff\" if is_playoff else \"regular\")] + data_values\n",
    "        data.append(data_values)\n",
    "\n",
    "    if col_names and data:\n",
    "        return pd.DataFrame(data, columns=col_names)\n",
    "    else:\n",
    "        print(\"No data found\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_top_diddy(link_dict: dict, sec_delay: int) -> pd.DataFrame:\n",
    "    result_df = []\n",
    "\n",
    "    for name,link in link_dict.items():\n",
    "        print(f\"Scraping: {name}\")\n",
    "        time.sleep(sec_delay)\n",
    "\n",
    "        playoff_df = scrape_totals(link=link, player_name=name, is_playoff=True)\n",
    "\n",
    "        time.sleep(sec_delay)\n",
    "        regular_df = scrape_totals(link=link, player_name=name, is_playoff=False)\n",
    "\n",
    "        result_df.append(playoff_df)\n",
    "        result_df.append(regular_df)\n",
    "\n",
    "    return pd.concat(result_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Dwayne Wade\n",
      "Scraping: Kobe Bryant\n",
      "Scraping: Lamar Odom\n",
      "Scraping: Pau Gasol\n",
      "Scraping: Derek Fisher\n",
      "Scraping: Lebron James\n",
      "Scraping: Jordan Farmar\n",
      "Scraping: Sasha Vujačić\n",
      "Scraping: Trevor Ariza\n",
      "Scraping: Udonis Haslem\n",
      "Scraping: Andrew Bynum\n",
      "Scraping: Jason Kidd\n",
      "Scraping: Kevin Garnett\n",
      "Scraping: Luke Walton\n",
      "Scraping: Paul Pierce\n",
      "Scraping: Rajon Rondo\n",
      "Scraping: Ray Allen\n",
      "Scraping: Shannon Brown\n",
      "Scraping: J.R. Smith\n",
      "Scraping: Chris Bosh\n"
     ]
    }
   ],
   "source": [
    "top_df = scrape_top_diddy(link_dict=top_20_player_links, sec_delay=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_df_path = \"./Diddy-Stats/top20.csv\"\n",
    "top_df.to_csv(top_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape a season \n",
    "* Input player id, year, and season_type -> get the data for that season\n",
    "    * year is the later one, so 2011-12 would be the integer 2012\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_ids = {\n",
    "    \"Dwyane Wade\": \"p/piercpa01\",\n",
    "    \"Paul Pierce\": \"w/wadedw01\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_playoffs(url):\n",
    "    # pgl_basic_playoffs\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Error: {response.status_code}\")\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    table_id = \"pgl_basic_playoffs\"\n",
    "    table = soup.find('table', id = table_id)\n",
    "\n",
    "    if table is None:\n",
    "        raise Exception(\"Table not found\")\n",
    "    \n",
    "    table_body = table.find('tbody')\n",
    "\n",
    "    if table_body is None:\n",
    "        raise Exception(\"Table Body not found\")\n",
    "    \n",
    "    # Selecting all rows that don't have the 'thead' class attribute\n",
    "    table_rows = table_body.find_all('tr', class_=lambda x: x != 'thead')\n",
    "\n",
    "    if table_rows is None:\n",
    "        raise Exception(\"No rows found\")\n",
    "    \n",
    "    data = []\n",
    "    col_names = None\n",
    "    \n",
    "    for row in table_rows:\n",
    "        table_data = [td for td in row if \"right\" in td['class']]\n",
    "\n",
    "        if col_names is None:\n",
    "            col_names = [td['data-stat'] for td in table_data if 'data-stat' in td.attrs]\n",
    "        \n",
    "        data_values = [td.text.strip() for td in table_data]\n",
    "        data.append(data_values)\n",
    "    \n",
    "    if col_names and data:\n",
    "        return pd.DataFrame(data, columns=col_names)\n",
    "    else:\n",
    "        raise Exception(\"No data found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_playoffs(url, year):\n",
    "    df = scrape_all_playoffs(url)\n",
    "\n",
    "    df.loc[df[\"year\"] == year]\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_regular(url):\n",
    "    # pgl_basic\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_season(player_id, year, season_type): \n",
    "    url = f\"https://www.basketball-reference.com/players/{player_id}\"\n",
    "\n",
    "    df = None\n",
    "    if season_type == \"playoffs\":\n",
    "        url += \"/gamelog-playoffs/\"\n",
    "        df = scrape_playoffs(url, year)\n",
    "\n",
    "\n",
    "    elif season_type == \"regular\":\n",
    "        url += f\"/gamelog/{year}\" \n",
    "        df = scrape_regular(url)\n",
    "\n",
    "    \n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
