{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Diddy Sheets\n",
    "First column = date\n",
    "Second column = home team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas scikit-learn bs4 requests\n",
    "\n",
    "import pandas as pd \n",
    "from datetime import datetime, date\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opp_team(soup: BeautifulSoup) -> str: \n",
    "    scorebox_div = soup.find('div', class_=\"scorebox\")\n",
    "    if scorebox_div is None:\n",
    "        print(\"Not Found\")\n",
    "        return None\n",
    "    \n",
    "    link_tag = scorebox_div.find('strong').find('a')\n",
    "    if link_tag is None:\n",
    "        print(\"<a> tag not found\")\n",
    "        return None\n",
    "\n",
    "    if 'href' not in link_tag.attrs:\n",
    "        print(\"Link not found in href\")\n",
    "        return None\n",
    "    \n",
    "    href = link_tag[\"href\"]\n",
    "\n",
    "    return href.split(\"/\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_headers(soup: BeautifulSoup, table_id: str) -> str:\n",
    "    table = soup.find('table', table_id)\n",
    "\n",
    "    if table is None:\n",
    "        print(\"Couldn't find the table\")\n",
    "        return None\n",
    "    \n",
    "    table_header = table.find('thead')\n",
    "\n",
    "    if table_header is None:\n",
    "        print(\"Couldn't find the table header\")\n",
    "    \n",
    "    data_stats = [th['data-stat'] for th in table.find_all('th') if 'data_stat' in th.attrs]\n",
    "\n",
    "    return data_stats[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stat_table(soup: BeautifulSoup, table_id: str, team: str) -> pd.DataFrame:\n",
    "    table = soup.find('table', id = table_id)\n",
    "\n",
    "    if table is None:\n",
    "        print(\"Table not found\")\n",
    "        return None\n",
    "    \n",
    "    table_body = table.find('tbody')\n",
    "\n",
    "    if table_body is None:\n",
    "        print(\"Table body not found\")\n",
    "        return None\n",
    "\n",
    "    table_rows = [tr for tr in table_body.find_all('tr') if 'thead' not in tr.attrs]\n",
    "\n",
    "    if table_rows is None:\n",
    "        print(\"Table rows not found\")\n",
    "        return None\n",
    "\n",
    "    data = []\n",
    "    col_names = None\n",
    "\n",
    "    for row in table_rows:\n",
    "        table_data = [td for td in row if td.name == \"td\"]\n",
    "\n",
    "        if \"Did Not Play\" in str(row) or \"Reserves\" in str(row): \n",
    "            continue \n",
    "    \n",
    "        player_name = row.find('th').text.strip()\n",
    "\n",
    "        if col_names is None:\n",
    "            col_names = [\"player\", \"team\"] + [td['data-stat'] for td in table_data if 'data-stat' in td.attrs]\n",
    "        \n",
    "        data_values = [td.text.strip() for td in table_data]\n",
    "        data_values = [player_name, team] + data_values\n",
    "        data.append(data_values)\n",
    "\n",
    "    if col_names and data:\n",
    "        return pd.DataFrame(data, columns=col_names)\n",
    "    else:\n",
    "        print(\"No data found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_date(df: pd.DataFrame, date: date):\n",
    "    df['date'] = date\n",
    "    return df\n",
    "\n",
    "def correct_date(df: pd.DataFrame):\n",
    "    if \"date\" not in df.columns:\n",
    "        print(\"Dates not found. Dataframe must contain a column named 'date'\")\n",
    "        return None\n",
    "    \n",
    "    if pd.api.types.is_string_dtype(df['date']):\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the home team and the day the game was played\n",
    "def scrape_game(home_team: str, game_date: date, sec_delay: int):\n",
    "    print(f\"Scraping: {home_team}\")\n",
    "\n",
    "    time.sleep(sec_delay)\n",
    "\n",
    "    date_str = game_date.strftime(\"%Y%m%d\")\n",
    "    link = f\"https://www.basketball-reference.com/boxscores/{date_str}0{home_team}.html\"\n",
    "\n",
    "    response = requests.get(link)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Not Found: \" + link)\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    opp_team = get_opp_team(soup)\n",
    "\n",
    "    home_table_id = f\"box-{home_team}-game-basic\"\n",
    "    opponent_table_id = f\"box-{opp_team}-game-basic\"\n",
    "\n",
    "    home_df = get_stat_table(soup, home_table_id, home_team)\n",
    "    home_df = add_date(home_df, game_date)\n",
    "    opp_df = get_stat_table(soup, opponent_table_id, opp_team)\n",
    "    opp_df = add_date(opp_df, game_date)\n",
    "\n",
    "    return pd.concat([home_df, opp_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_sheets(df: pd.DataFrame, sec_delay: int) -> pd.DataFrame:\n",
    "    all_games_df = df.apply(lambda row: scrape_game(row['home_team'], row['date'], sec_delay=sec_delay), axis = 1).tolist()\n",
    "    \n",
    "    return pd.concat(all_games_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the diddy csv\n",
    "diddy_path = \"./Celeb-Attendence/diddy.csv\"\n",
    "\n",
    "diddy_csv = pd.read_csv(diddy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diddy_csv.head()\n",
    "type(diddy_csv['date'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "diddy_csv = correct_date(diddy_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diddy_games_df = scrape_sheets(diddy_csv, sec_delay=5)\n",
    "diddy_games_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "diddy_save_path = \"./Diddy-Stats/diddygames.csv\"\n",
    "\n",
    "diddy_games_df.to_csv(diddy_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Career Stats of the Top 20 Diddy players\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_player_links = {\n",
    "    \"Dwyane Wade\": \"https://www.basketball-reference.com/players/w/wadedw01.html\",\n",
    "    \"Kobe Bryant\": \"https://www.basketball-reference.com/players/b/bryanko01.html\",\n",
    "    \"Lamar Odom\": \"https://www.basketball-reference.com/players/o/odomla01.html\",\n",
    "    \"Pau Gasol\": \"https://www.basketball-reference.com/players/g/gasolpa01.html\",\n",
    "    \"Derek Fisher\": \"https://www.basketball-reference.com/players/f/fishede01.html\",\n",
    "    \"Lebron James\": \"https://www.basketball-reference.com/players/j/jamesle01.html\",\n",
    "    \"Jordan Farmar\": \"https://www.basketball-reference.com/players/f/farmajo01.html\",\n",
    "    \"Sasha Vujačić\": \"https://www.basketball-reference.com/players/v/vujacsa01.html\",\n",
    "    \"Trevor Ariza\": \"https://www.basketball-reference.com/players/a/arizatr01.html\",\n",
    "    \"Udonis Haslem\": \"https://www.basketball-reference.com/players/h/hasleud01.html\",\n",
    "    \"Andrew Bynum\": \"https://www.basketball-reference.com/players/b/bynuman01.html\",\n",
    "    \"Jason Kidd\": \"https://www.basketball-reference.com/players/k/kiddja01.html\",\n",
    "    \"Kevin Garnett\": \"https://www.basketball-reference.com/players/g/garneke01.html\",\n",
    "    \"Luke Walton\": \"https://www.basketball-reference.com/players/w/waltolu01.html\",\n",
    "    \"Paul Pierce\": \"https://www.basketball-reference.com/players/p/piercpa01.html\",\n",
    "    \"Rajon Rondo\": \"https://www.basketball-reference.com/players/r/rondora01.html\",\n",
    "    \"Ray Allen\": \"https://www.basketball-reference.com/players/a/allenra02.html\",\n",
    "    \"Shannon Brown\": \"https://www.basketball-reference.com/players/b/brownsh01.html\",\n",
    "    \"J.R. Smith\": \"https://www.basketball-reference.com/players/s/smithjr01.html\",\n",
    "    \"Chris Bosh\": \"https://www.basketball-reference.com/players/b/boshch01.html\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New setup for the tables in basketball reference\n",
    "* regular season is in the div with id=\"div_per_game_stats\"\n",
    "* post season is in the div with id=\"div_per_game_stats_post\"\n",
    "\n",
    "For total stats\n",
    "* regular = id=\"div_totals\"\n",
    "* playoffs = \"div_playoffs_totals\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_totals(link: str, player_name: str, is_playoff: bool = False) -> pd.DataFrame:\n",
    "    response = requests.get(link)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"Not Found: \" + link)\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    if is_playoff:\n",
    "        div_id = \"div_playoffs_totals\"\n",
    "    else:\n",
    "        div_id = \"div_totals\"\n",
    "\n",
    "    div = soup.find('div', id=div_id)\n",
    "    table = div.find('table')\n",
    "\n",
    "    table_body = table.find('tbody')\n",
    "    table_rows = table_body.find_all('tr')\n",
    "    \n",
    "    data = []\n",
    "    col_names = None\n",
    "\n",
    "    for row in table_rows:\n",
    "        table_data = [td for td in row if td.name == \"td\"]\n",
    "\n",
    "        season = row.find('th').text.strip()\n",
    "\n",
    "        if col_names is None:\n",
    "            col_names = [\"player\", \"season\", \"season_type\"] + [td['data-stat'] for td in table_data if 'data-stat' in td.attrs]\n",
    "        \n",
    "        data_values = [td.text.strip() for td in table_data]\n",
    "        data_values = [player_name, season, (\"playoff\" if is_playoff else \"regular\")] + data_values\n",
    "        data.append(data_values)\n",
    "\n",
    "    if col_names and data:\n",
    "        return pd.DataFrame(data, columns=col_names)\n",
    "    else:\n",
    "        print(\"No data found\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_top_diddy(link_dict: dict, sec_delay: int) -> pd.DataFrame:\n",
    "    result_df = []\n",
    "\n",
    "    for name,link in link_dict.items():\n",
    "        print(f\"Scraping: {name}\")\n",
    "        time.sleep(sec_delay)\n",
    "\n",
    "        playoff_df = scrape_totals(link=link, player_name=name, is_playoff=True)\n",
    "\n",
    "        time.sleep(sec_delay)\n",
    "        regular_df = scrape_totals(link=link, player_name=name, is_playoff=False)\n",
    "\n",
    "        result_df.append(playoff_df)\n",
    "        result_df.append(regular_df)\n",
    "\n",
    "    return pd.concat(result_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Dwayne Wade\n",
      "Scraping: Kobe Bryant\n",
      "Scraping: Lamar Odom\n",
      "Scraping: Pau Gasol\n",
      "Scraping: Derek Fisher\n",
      "Scraping: Lebron James\n",
      "Scraping: Jordan Farmar\n",
      "Scraping: Sasha Vujačić\n",
      "Scraping: Trevor Ariza\n",
      "Scraping: Udonis Haslem\n",
      "Scraping: Andrew Bynum\n",
      "Scraping: Jason Kidd\n",
      "Scraping: Kevin Garnett\n",
      "Scraping: Luke Walton\n",
      "Scraping: Paul Pierce\n",
      "Scraping: Rajon Rondo\n",
      "Scraping: Ray Allen\n",
      "Scraping: Shannon Brown\n",
      "Scraping: J.R. Smith\n",
      "Scraping: Chris Bosh\n"
     ]
    }
   ],
   "source": [
    "top_df = scrape_top_diddy(link_dict=top_20_player_links, sec_delay=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_df_path = \"./Diddy-Stats/top20.csv\"\n",
    "top_df.to_csv(top_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape a season \n",
    "* Input player id, year, and season_type -> get the data for that season\n",
    "    * year is the later one, so 2011-12 would be the integer 2012\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_ids = {\n",
    "    \"Dwyane Wade\": \"p/piercpa01\",\n",
    "    \"Paul Pierce\": \"w/wadedw01\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_playoffs(url):\n",
    "    # pgl_basic_playoffs\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Error: \" + str(response.status_code))\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    table_id = \"pgl_basic_playoffs\"\n",
    "    table = soup.find('table', id = table_id)\n",
    "\n",
    "    if table is None:\n",
    "        raise Exception(\"Table not found\")\n",
    "    \n",
    "    table_body = table.find('tbody')\n",
    "\n",
    "    if table_body is None:\n",
    "        raise Exception(\"Table Body not found\")\n",
    "    \n",
    "    # Selecting all rows that don't have the 'thead' class attribute\n",
    "    table_rows = table_body.find_all('tr', class_=lambda x: x != 'thead')\n",
    "\n",
    "    if table_rows is None:\n",
    "        raise Exception(\"No rows found\")\n",
    "    \n",
    "    data = []\n",
    "    col_names = None\n",
    "    \n",
    "    for row in table_rows:\n",
    "        table_data = [td for td in row if \"right\" in td['class'] or (\"left\" in td['class'] and td[\"data-stat\"] == \"date_game\")]\n",
    "\n",
    "        if col_names is None:\n",
    "            col_names = [td['data-stat'] for td in table_data if 'data-stat' in td.attrs]\n",
    "        \n",
    "        data_values = [td.text.strip() for td in table_data]\n",
    "        data.append(data_values)\n",
    "    \n",
    "    if col_names and data:\n",
    "        df = pd.DataFrame(data, columns=col_names)\n",
    "\n",
    "        # dropping columns \n",
    "        # Don't need ranker, game_season, gs\n",
    "        df = df.drop(['ranker', 'game_season', 'gs'], axis=1)\n",
    "        return df\n",
    "    else:\n",
    "        raise Exception(\"No data found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_playoffs(url, year):\n",
    "    df = scrape_all_playoffs(url)\n",
    "\n",
    "    # changing to date time format\n",
    "    df['date_game'] = pd.to_datetime(df['date_game'])\n",
    "\n",
    "    # filtering year \n",
    "    df = df.loc[df['date_game'].dt.year == year]\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_regular(url):\n",
    "    # pgl_basic\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error: {response.status_code}\")\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    table_id = \"pgl_basic\"\n",
    "    table = soup.find('table', id = table_id)\n",
    "\n",
    "    if table is None:\n",
    "        raise Exception(\"Table not found\")\n",
    "    \n",
    "    table_body = table.find('tbody')\n",
    "\n",
    "    if table_body is None:\n",
    "        raise Exception(\"Table Body not found\")\n",
    "    \n",
    "    # Selecting all rows that don't have the 'thead' class attribute\n",
    "    table_rows = table_body.find_all('tr', class_=lambda x: x != 'thead')\n",
    "\n",
    "    if table_rows is None:\n",
    "        raise Exception(\"No rows found\")\n",
    "    \n",
    "    data = []\n",
    "    col_names = None\n",
    "    \n",
    "    for row in table_rows:\n",
    "        if \"Did Not Play\" in str(row) or \"Not With Team\" in str(row) or \"Inactive\" in str(row) or \"Did Not Dress\" in str(row): \n",
    "            continue \n",
    "\n",
    "        if col_names is None:\n",
    "            col_names = [td['data-stat'] for td in row if 'data-stat' in td.attrs]\n",
    "\n",
    "        data_values = [td.text.strip() for td in row]\n",
    "        data.append(data_values)\n",
    "    \n",
    "    if col_names and data:\n",
    "        df = pd.DataFrame(data, columns=col_names)\n",
    "\n",
    "        # dropping columns \n",
    "        df = df.drop(['ranker','game_season', 'gs', 'game_location', 'age'], axis=1)\n",
    "        return df\n",
    "    else:\n",
    "        raise Exception(\"No data found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_regular_years(player_id):\n",
    "    url = f\"https://www.basketball-reference.com/players/{player_id}.html\"\n",
    "\n",
    "    response = requests.get(url) \n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error: {response.status_code}\")\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser') \n",
    "\n",
    "    inner_nav_div = soup.find('div', id='inner_nav')\n",
    "    if inner_nav_div is None:\n",
    "        raise Exception(\"inner_nav div not found\")\n",
    "    \n",
    "    a_tags = inner_nav_div.find_all('a')\n",
    "    \n",
    "    if a_tags is None:\n",
    "        raise Exception(\"a tags not found\")\n",
    "    \n",
    "    game_log_hrefs = [a[\"href\"] for a in a_tags \n",
    "                      if 'href' in a.attrs \n",
    "                      and 'gamelog' in a['href']\n",
    "                      and 'playoff' not in a['href']]\n",
    "\n",
    "    return list(set(game_log_hrefs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_regular(player_name, player_id, sec_delay):\n",
    "    url_suffixes = get_all_regular_years(player_id)\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for suffix in url_suffixes:\n",
    "        url = f\"https://www.basketball-reference.com/{suffix}\"\n",
    "\n",
    "        df = scrape_regular(url)\n",
    "        prev_year = str(int(suffix[-4:])-1)\n",
    "        season = prev_year + \"-\" + suffix[-4:]\n",
    "        df[\"season\"] = season\n",
    "\n",
    "        df_list.append(df)\n",
    "        time.sleep(sec_delay)\n",
    "\n",
    "    result_df = pd.concat(df_list, ignore_index=True)\n",
    "    result_df['player'] = player_name\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_season(player_name, player_id, year, season_type): \n",
    "    url = f\"https://www.basketball-reference.com/players/{player_id}\"\n",
    "\n",
    "    df = None\n",
    "    if season_type == \"playoffs\":\n",
    "        url += \"/gamelog-playoffs/\"\n",
    "        df = scrape_playoffs(url, year)\n",
    "\n",
    "    elif season_type == \"regular\":\n",
    "        url += f\"/gamelog/{year}\" \n",
    "        df = scrape_regular(url)\n",
    "\n",
    "    df['player'] = player_name\n",
    "    df['season_type'] = season_type\n",
    "\n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Players Playoff and Regular season stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_playoffs_df = scrape_all_playoffs(\"https://www.basketball-reference.com/players/p/piercpa01/gamelog-playoffs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_playoffs_df['player'] = \"Paul Pierce\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_playoffs_path = \"./career_data/paul_pierce_playoffs.csv\"\n",
    "pp_playoffs_df.to_csv(pp_playoffs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wade_regular_df = scrape_all_regular(\"Dwyane Wade\", \"w/wadedw01\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wade_regular_path = \"./career_data/dwyane_wade_regular.csv\"\n",
    "wade_regular_df.to_csv(wade_regular_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_players = {\n",
    "    \"Dwyane Wade\": \"p/piercpa01\",\n",
    "    \"Paul Pierce\": \"w/wadedw01\",\n",
    "    \"Kevin Garnett\": \"g/garneke01\",\n",
    "    \"Rajon Rondo\": \"r/rondora01\",\n",
    "    \"Pau Gasol\": \"g/gasolpa01\",\n",
    "    \"Lebron James\": \"j/jamesle01\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dwyane Wade\n",
      "Paul Pierce\n",
      "Kevin Garnett\n",
      "Rajon Rondo\n",
      "Pau Gasol\n",
      "Lebron James\n"
     ]
    }
   ],
   "source": [
    "for key, value in sim_players.items(): \n",
    "    print(key)\n",
    "    save_path = \"./career_data/\" + key.split(\" \")[0].lower() + \"_\" + key.split(\" \")[1].lower()\n",
    "    \n",
    "    if not os.path.isfile(save_path + \"_regular.csv\"):\n",
    "        reg_df = scrape_all_regular(key, value, sec_delay=10)\n",
    "\n",
    "        reg_df.to_csv(save_path + \"_regular.csv\")\n",
    "\n",
    "    if not os.path.isfile(save_path + \"_playoffs.csv\"):\n",
    "        playoff_url = f\"https://www.basketball-reference.com/players/{value}/gamelog-playoffs/\"\n",
    "        playoff_df = scrape_all_playoffs(playoff_url)\n",
    "\n",
    "        playoff_df['player'] = key\n",
    "\n",
    "        playoff_df.to_csv(save_path + \"_playoffs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = scrape_all_playoffs(\"https://www.basketball-reference.com/players/w/wadedw01/gamelog-playoffs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['player'] = \"Dwyane Wade\"\n",
    "test_df.to_csv('./career_data/dwyane_wade_playoffs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Drizzy Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from datetime import datetime, date\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drizzy_df = pd.read_csv(\"Celeb-Attendence/drizzy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(drizzy_df[\"date\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing duplicates in drizzy_df\n",
    "# drizzy_df = drizzy_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drizzy_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drizzy_df.to_csv(\"Celeb-Attendence/drizzy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6l/1snc6g9s7dg27ls61y9tlmsw0000gn/T/ipykernel_5199/1358148887.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: HOU\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: LAC\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: GSW\n",
      "Scraping: TOR\n",
      "Scraping: LAC\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: LAL\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: LAL\n",
      "Scraping: GSW\n",
      "Scraping: TOR\n",
      "Scraping: LAL\n",
      "Scraping: MIA\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Not Found: https://www.basketball-reference.com/boxscores/202110110TOR.html\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: LAL\n",
      "Scraping: GSW\n",
      "Scraping: MIA\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Not Found: https://www.basketball-reference.com/boxscores/202203230TOR.html\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: LAL\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: MIA\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: BKN\n",
      "Not Found: https://www.basketball-reference.com/boxscores/201405020BKN.html\n",
      "Scraping: NYK\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: MEM\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: WAS\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Not Found: https://www.basketball-reference.com/boxscores/202210090TOR.html\n",
      "Scraping: OKC\n",
      "Scraping: LAL\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: GSW\n",
      "Scraping: SAC\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: MIA\n",
      "Scraping: MIA\n",
      "Scraping: LAL\n",
      "Scraping: TOR\n",
      "Scraping: MIA\n",
      "Scraping: MIA\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: MIA\n",
      "Scraping: LAL\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: GSW\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: LAC\n",
      "Scraping: LAC\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Scraping: TOR\n",
      "Not Found: https://www.basketball-reference.com/boxscores/201310230TOR.html\n",
      "Scraping: SAC\n",
      "Scraping: NYK\n",
      "Scraping: GSW\n",
      "Scraping: MIA\n",
      "Scraping: TOR\n",
      "Scraping: LAC\n",
      "Scraping: NYK\n",
      "Scraping: SAC\n"
     ]
    }
   ],
   "source": [
    "drizzy_df = correct_date(drizzy_df)\n",
    "\n",
    "drizzy_sheets_df = scrape_sheets(drizzy_df, sec_delay=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "drizzy_sheets_df.to_csv(\"Drizzy-Stats/drizzygames.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
