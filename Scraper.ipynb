{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Diddy Sheets\n",
    "First column = date\n",
    "Second column = home team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas scikit-learn bs4 requests\n",
    "\n",
    "import pandas as pd \n",
    "from datetime import datetime, date\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opp_team(soup: BeautifulSoup) -> str: \n",
    "    scorebox_div = soup.find('div', class_=\"scorebox\")\n",
    "    if scorebox_div is None:\n",
    "        print(\"Not Found\")\n",
    "        return None\n",
    "    \n",
    "    link_tag = scorebox_div.find('strong').find('a')\n",
    "    if link_tag is None:\n",
    "        print(\"<a> tag not found\")\n",
    "        return None\n",
    "\n",
    "    if 'href' not in link_tag.attrs:\n",
    "        print(\"Link not found in href\")\n",
    "        return None\n",
    "    \n",
    "    href = link_tag[\"href\"]\n",
    "\n",
    "    return href.split(\"/\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_headers(soup: BeautifulSoup, table_id: str) -> str:\n",
    "    table = soup.find('table', table_id)\n",
    "\n",
    "    if table is None:\n",
    "        print(\"Couldn't find the table\")\n",
    "        return None\n",
    "    \n",
    "    table_header = table.find('thead')\n",
    "\n",
    "    if table_header is None:\n",
    "        print(\"Couldn't find the table header\")\n",
    "    \n",
    "    data_stats = [th['data-stat'] for th in table.find_all('th') if 'data_stat' in th.attrs]\n",
    "\n",
    "    return data_stats[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stat_table(soup: BeautifulSoup, table_id: str, team: str) -> pd.DataFrame:\n",
    "    table = soup.find('table', id = table_id)\n",
    "\n",
    "    if table is None:\n",
    "        print(\"Table not found\")\n",
    "        return None\n",
    "    \n",
    "    table_body = table.find('tbody')\n",
    "\n",
    "    if table_body is None:\n",
    "        print(\"Table body not found\")\n",
    "        return None\n",
    "\n",
    "    table_rows = [tr for tr in table_body.find_all('tr') if 'thead' not in tr.attrs]\n",
    "\n",
    "    if table_rows is None:\n",
    "        print(\"Table rows not found\")\n",
    "        return None\n",
    "\n",
    "    data = []\n",
    "    col_names = None\n",
    "\n",
    "    for row in table_rows:\n",
    "        table_data = [td for td in row if td.name == \"td\"]\n",
    "\n",
    "        if \"Did Not Play\" in str(row) or \"Reserves\" in str(row): \n",
    "            continue \n",
    "    \n",
    "        player_name = row.find('th').text.strip()\n",
    "\n",
    "        if col_names is None:\n",
    "            col_names = [\"player\", \"team\"] + [td['data-stat'] for td in table_data if 'data-stat' in td.attrs]\n",
    "        \n",
    "        data_values = [td.text.strip() for td in table_data]\n",
    "        data_values = [player_name, team] + data_values\n",
    "        data.append(data_values)\n",
    "\n",
    "    if col_names and data:\n",
    "        return pd.DataFrame(data, columns=col_names)\n",
    "    else:\n",
    "        print(\"No data found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_date(df: pd.DataFrame, date: date):\n",
    "    df['date'] = date\n",
    "    return df\n",
    "\n",
    "def correct_date(df: pd.DataFrame):\n",
    "    if \"date\" not in df.columns:\n",
    "        print(\"Dates not found. Dataframe must contain a column named 'date'\")\n",
    "        return None\n",
    "    \n",
    "    if pd.api.types.is_string_dtype(df['date']):\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the home team and the day the game was played\n",
    "def scrape_game(home_team: str, game_date: date, sec_delay: int):\n",
    "    print(f\"Scraping: {home_team}\")\n",
    "\n",
    "    time.sleep(sec_delay)\n",
    "\n",
    "    date_str = game_date.strftime(\"%Y%m%d\")\n",
    "    link = f\"https://www.basketball-reference.com/boxscores/{date_str}0{home_team}.html\"\n",
    "\n",
    "    response = requests.get(link)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Not Found: \" + link)\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    opp_team = get_opp_team(soup)\n",
    "\n",
    "    home_table_id = f\"box-{home_team}-game-basic\"\n",
    "    opponent_table_id = f\"box-{opp_team}-game-basic\"\n",
    "\n",
    "    home_df = get_stat_table(soup, home_table_id, home_team)\n",
    "    home_df = add_date(home_df, game_date)\n",
    "    opp_df = get_stat_table(soup, opponent_table_id, opp_team)\n",
    "    opp_df = add_date(opp_df, game_date)\n",
    "\n",
    "    return pd.concat([home_df, opp_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_sheets(df: pd.DataFrame, sec_delay: int) -> pd.DataFrame:\n",
    "    all_games_df = df.apply(lambda row: scrape_game(row['home_team'], row['date'], sec_delay=sec_delay), axis = 1).tolist()\n",
    "    \n",
    "    return pd.concat(all_games_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the diddy csv\n",
    "diddy_path = \"./Celeb-Attendence/diddy.csv\"\n",
    "\n",
    "diddy_csv = pd.read_csv(diddy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diddy_csv.head()\n",
    "type(diddy_csv['date'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "diddy_csv = correct_date(diddy_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: LAL\n"
     ]
    }
   ],
   "source": [
    "diddy_games_df = scrape_sheets(diddy_csv, sec_delay=5)\n",
    "diddy_games_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diddy_save_path = \"./Diddy-Stats/diddygames.csv\"\n",
    "\n",
    "diddy_games_df.to_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
